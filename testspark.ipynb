{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!sudo apt-get update -qq\n",
    "!sudo apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "# Download and extract PySpark 3.5.2\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.5.2/spark-3.5.2-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.2-bin-hadoop3.tgz\n",
    "\n",
    "# Install findspark\n",
    "!pip install -q findspark\n",
    "!pip install pyspark\n",
    "\n",
    "import os\n",
    "\n",
    "# Set environment variables for Java and Spark\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"./spark-3.5.2-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_connection():\n",
    "    s_conn = None\n",
    "\n",
    "    try:\n",
    "        s_conn = SparkSession.builder \\\n",
    "            .appName('SparkDataStreaming') \\\n",
    "            .config(\"spark.network.timeout\", \"800s\") \\\n",
    "            .config(\"spark.jars.packages\", \"com.github.jnr:jnr-posix:3.1.15,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.2\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        s_conn.sparkContext.setLogLevel(\"ERROR\")\n",
    "        logging.info(\"Spark connection created successfully!\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Couldn't create the spark session due to exception {e}\")\n",
    "\n",
    "    return s_conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_kafka(spark_conn):\n",
    "    spark_df = None\n",
    "    try:\n",
    "        spark_df = spark_conn.readStream \\\n",
    "            .format('kafka') \\\n",
    "            .option('kafka.bootstrap.servers', '44.215.213.113:9092') \\\n",
    "            .option('subscribe', 'e-commerce') \\\n",
    "            .option('startingOffsets', 'earliest') \\\n",
    "            .load()\n",
    "        logging.info(\"kafka dataframe created successfully\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"kafka dataframe could not be created because: {e}\")\n",
    "\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_selection_df_from_kafka(spark_df):\n",
    "    schema = StructType([\n",
    "        StructField(\"product_id\", StringType(), False),\n",
    "        StructField(\"product_name\", StringType(), False),\n",
    "        StructField(\"category\", StringType(), False),\n",
    "        StructField(\"discounted_price\", StringType(), False),\n",
    "        StructField(\"actual_price\", StringType(), False),\n",
    "        StructField(\"discount_percentage\", StringType(), False),\n",
    "        StructField(\"rating\", StringType(), False),\n",
    "        StructField(\"rating_count\", StringType(), False),\n",
    "        StructField(\"about_product\", StringType(), False),\n",
    "        StructField(\"user_id\", StringType(), False),\n",
    "        StructField(\"user_name\", StringType(), False),\n",
    "        StructField(\"review_id\", StringType(), False),\n",
    "        StructField(\"review_title\", StringType(), False),\n",
    "        StructField(\"review_content\", StringType(), False),\n",
    "        StructField(\"img_link\", StringType(), False),\n",
    "        StructField(\"product_link\", StringType(), False)\n",
    "    ])\n",
    "\n",
    "    sel = spark_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "        .select(from_json(col('value'), schema).alias('data')).select(\"data.*\")\n",
    "\n",
    "    return sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_stream_to_console(selection_df):\n",
    "    try:\n",
    "        query = selection_df.writeStream \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .format(\"console\") \\\n",
    "            .start()\n",
    "\n",
    "        query.awaitTermination()\n",
    "        logging.info(\"Stream started successfully and writing to console\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Stream could not be started due to exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark_conn = create_spark_connection()\n",
    "    if spark_conn is not None:\n",
    "        spark_df = connect_to_kafka(spark_conn)\n",
    "        if spark_df is not None:\n",
    "            selection_df = create_selection_df_from_kafka(spark_df)\n",
    "            write_stream_to_console(selection_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
